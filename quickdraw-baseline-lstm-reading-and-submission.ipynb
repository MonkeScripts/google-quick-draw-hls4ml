{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"8b08fbab2000a563b388f126eac74362641e497c","trusted":true},"outputs":[],"source":["batch_size = 1024\n","STROKE_COUNT = 196\n","TRAIN_SAMPLES = 750\n","VALID_SAMPLES = 75\n","TEST_SAMPLES = 50"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%matplotlib inline\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from keras.utils import to_categorical\n","# from keras.utils.np_utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","from keras.metrics import top_k_categorical_accuracy\n","def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n","from glob import glob\n","import gc\n","gc.enable()\n","# def get_available_gpus():\n","#     from tensorflow.python.client import device_lib\n","#     local_device_protos = device_lib.list_local_devices()\n","#     return [x.name for x in local_device_protos if x.device_type == 'GPU']\n","\n","#List gpus on computer\n","gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n","print(gpu_devices)\n","for device in gpu_devices:\n","    #restrict gpu usage \n","    tf.config.experimental.set_memory_growth(device, True)\n","base_dir = os.path.join('..', 'input')\n","print(base_dir)\n","test_path = os.path.join(base_dir, 'test_simplified.csv')\n","print(test_path)"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"7acacf8e960084782425ef1a1a3fd532a240ad48","trusted":true},"outputs":[],"source":["from ast import literal_eval\n","#gets the path to training data\n","ALL_TRAIN_PATHS = glob(os.path.join(base_dir, 'train_simplified_animals','train_simplified', '*.csv'))\n","COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n","\n","#takes in a string of strokes\n","# \"[ \n","#   [  // First stroke evarious pixels that are highlighted\n","#     [x0, x1, x2, x3, ...],\n","#     [y0, y1, y2, y3, ...],\n","#     [t0, t1, t2, t3, ...]\n","#   ],\n","#   [  // Second stroke\n","#     [x0, x1, x2, x3, ...],\n","#     [y0, y1, y2, y3, ...],\n","#     [t0, t1, t2, t3, ...]\n","#   ],\n","#   ... // Additional strokes\n","# ]\"\n","def _stack_it(raw_strokes):\n","    \"\"\"preprocess the string and make \n","    a standard Nx3 stroke vector\"\"\"\n","    #converts the stroke strings to a list\n","    stroke_vec = literal_eval(raw_strokes) # string->list\n","    # unwrap the list, converts it a tuple (x, y, i)\n","    # (x,y) are coordinates of each indiv pixel \n","    # and i is the index of each stroke\n","    in_strokes = [(xi,yi,i)  \n","     for i,(x,y) in enumerate(stroke_vec) \n","     for xi,yi in zip(x,y)]\n","    #create a nparray with all these indiv pixels \n","    #and their stroke ids\n","    c_strokes = np.stack(in_strokes)\n","    # replace stroke id with 1 for continue, 2 for new\n","    #changes the third column \n","    #prepends 1\n","    #np.diff() checks whether the pixels coordinates given \n","    #belong to the same stroke(same id)\n","    #used to determine when a new stroke began\n","    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n","    c_strokes[:,2] += 1 # since 0 is no stroke\n","    # pad the strokes with zeros\n","    #N x 3 swap to 3 X N then pad with zeros\n","    #later swap back to (N + padding) * 3\n","    return pad_sequences(c_strokes.swapaxes(0, 1), \n","                         maxlen=STROKE_COUNT, \n","                         padding='post').swapaxes(0, 1)\n","#load and process csv files\n","#5 samples in each train csv, from row 0 to 1000\n","def read_batch(samples=5, \n","               start_row=0,\n","               max_rows = 1000):\n","    \"\"\"\n","    load and process the csv files\n","    this function is horribly inefficient but simple\n","    \"\"\"\n","    out_df_list = []\n","    for c_path in ALL_TRAIN_PATHS:\n","        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n","        c_df.columns=COL_NAMES\n","        #picks out 5 samples of drawing and word for each object\n","        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n","    #concat all these samples into a full dataframe\n","    full_df = pd.concat(out_df_list)\n","    full_df['drawing'] = full_df['drawing'].\\\n","        map(_stack_it) #applies the stack function, returns padded strokes\n","    \n","    return full_df"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["#3 dict, maps the strings to numbers\n","train_args = dict(samples=TRAIN_SAMPLES, \n","                  start_row=0, \n","                  max_rows=int(TRAIN_SAMPLES*1.5))\n","valid_args = dict(samples=VALID_SAMPLES, \n","                  start_row=train_args['max_rows']+1, \n","                  max_rows=VALID_SAMPLES+25)\n","test_args = dict(samples=TEST_SAMPLES, \n","                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n","                 max_rows=TEST_SAMPLES+25)\n","train_df = read_batch(**train_args)\n","valid_df = read_batch(**valid_args)\n","test_df = read_batch(**test_args)\n","#create labels\n","word_encoder = LabelEncoder()\n","word_encoder.fit(train_df['word'])\n","print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"6d29237e-ece3-4dfd-9095-475296f4a608","_uuid":"8bae16a4973a215861fbb536a602c4f5abf3b4bf"},"source":["# Stroke-based Classification\n","Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff5ddced-d77e-473f-899d-82cf11ad2bd9","_uuid":"409468f1d5abd17b819482473a4f354a61f8d7ef","trusted":true},"outputs":[],"source":["def get_Xy(in_df):\n","    X = np.stack(in_df['drawing'], 0)\n","    y = to_categorical(word_encoder.transform(in_df['word'].values))\n","    return X, y\n","train_X, train_y = get_Xy(train_df)\n","valid_X, valid_y = get_Xy(valid_df)\n","test_X, test_y = get_Xy(test_df)\n","print(train_X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56240ed9-42b0-4f62-b3d1-f92017f04e30","_uuid":"5cc79204a0a1da048d1d58ba8dfdafd0af3ebcb8","trusted":true},"outputs":[],"source":["#3x3 canvas\n","fig, m_axs = plt.subplots(3,3, figsize = (96, 64))\n","#choose 9 random drawings\n","rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n","for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n","    test_arr = train_X[c_id]\n","    #filters out stroke = 0 (no stroke)\n","    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n","    lab_idx = np.cumsum(test_arr[:,2]-1) \n","    #cumulative sum -1, easier to distinguish each stroke\n","\n","    for i in np.unique(lab_idx):\n","        c_ax.plot(test_arr[lab_idx==i,0], \n","                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n","    c_ax.axis('off')\n","    c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d65490e7-302e-4232-afe7-4e9499010e31","_uuid":"ba9d55554ba9e4177df5f0645ca1e0f5e4393ca3","trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout \n","\n","stroke_read_model = Sequential()\n","stroke_read_model.add(BatchNormalization(input_shape = (196,)+train_X.shape[2:]))\n","stroke_read_model.add(Conv1D(48, (5,)))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(Conv1D(64, (5,)))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(Conv1D(96, (3,)))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(LSTM(128, return_sequences = True))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(LSTM(128, return_sequences = False))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(Dense(512))\n","stroke_read_model.add(Dropout(0.3))\n","stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n","stroke_read_model.compile(optimizer = 'adam', \n","                          loss = 'categorical_crossentropy', \n","                          metrics = ['categorical_accuracy', top_3_accuracy])\n","stroke_read_model.summary()\n","with open('stroke_read_model' + '.json', 'w') as outfile:\n","    outfile.write(stroke_read_model.to_json())"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a549512-a9d9-4afd-b748-3e1c3296e193","_uuid":"5fda10b30c47a8cf6ea822ed0a4a1d7cd2c81195","trusted":true},"outputs":[],"source":["import os\n","directory = \"weights/\"\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","weight_path=\"{}{}.weights.h5\".format(directory, 'stroke_lstm_model')\n","print(weight_path)\n","checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n","                             save_best_only=True, mode='min', save_weights_only = True)\n","\n","\n","reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n","                                   verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n","early = EarlyStopping(monitor=\"val_loss\", \n","                      mode=\"min\", \n","                      patience=5) \n","callbacks_list = [checkpoint, early, reduceLROnPlat]"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"825b3af8-9451-487b-a1e1-538f2f1489e1","_uuid":"ed2fc26af74aed1a93bbc253d61b72db5a81f5cc","trusted":true},"outputs":[],"source":["from IPython.display import clear_output\n","stroke_read_model.fit(train_X, train_y,\n","                      validation_data = (valid_X, valid_y), \n","                      batch_size = batch_size,\n","                      epochs = 400,\n","                      callbacks = callbacks_list)\n","clear_output()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7eb5b62-cf57-4380-8786-9ddc05be658f","_uuid":"858059b6c16d81f86460bef8fcf595e0d68d12b2","trusted":true},"outputs":[],"source":["stroke_read_model.load_weights(weight_path)\n","lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = 1024)\n","print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee75c585-b134-4ea2-b8f3-219e24efd1f1","_uuid":"6b9cdf52d233de60108d72f540db978801b578c1","trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","test_cat = np.argmax(test_y, 1)\n","pred_y = stroke_read_model.predict(test_X, batch_size = 1024)\n","pred_cat = np.argmax(pred_y, 1)\n","plt.matshow(confusion_matrix(test_cat, pred_cat))\n","print(classification_report(test_cat, pred_cat, \n","                            target_names = [x for x in word_encoder.classes_]))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"db1d371b-4b2c-478f-b6df-76db58a24fbe","_uuid":"bd9a16adcb46e07d7949644e69bf3483f7dce571"},"source":["# Reading Point by Point"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf7dff37-c634-4930-8dae-3dba8090c251","_uuid":"c43e87e7eccfb72dd35e64d872a7d658ffa535a3","scrolled":false,"trusted":true},"outputs":[],"source":["points_to_use = [5, 15, 20, 30, 40, 50]\n","points_to_user = [108]\n","samples = 12\n","word_dex = lambda x: word_encoder.classes_[x]\n","rand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\n","fig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples/8*24))\n","for c_id, c_axs in zip(rand_idxs, m_axs):\n","    res_idx = np.argmax(test_y[c_id])\n","    goal_cat = word_encoder.classes_[res_idx]\n","    \n","    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n","        test_arr = test_X[c_id, :].copy()\n","        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n","        stroke_pred = stroke_read_model.predict(np.expand_dims(test_arr,0))[0]\n","        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n","        top_10_sum = np.sum(stroke_pred[top_10_idx])\n","        \n","        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n","        lab_idx = np.cumsum(test_arr[:,2]-1)\n","        for i in np.unique(lab_idx):\n","            c_ax.plot(test_arr[lab_idx==i,0], \n","                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n","                      '.-')\n","        c_ax.axis('off')\n","        if pt_idx == (len(points_to_use)-1):\n","            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum))\n","        else:\n","            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum, \n","                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]/top_10_sum, \n","                                                                 100*stroke_pred[res_idx]/top_10_sum))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import hls4ml\n","config = hls4ml.utils.config_from_keras_model(stroke_read_model, granularity=\"name\")\n","print(\"-----------------------------------\")\n","print(\"Configuration\")\n","print(\"-----------------------------------\")\n","hls_model = hls4ml.converters.convert_from_keras_model(\n","    stroke_read_model,\n","    hls_config=config,\n","    output_dir=\"my-hls-test\",\n","    part=\"xc7a35tcpg236-1\",\n",")\n","#hls_model.compile()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hls_model.compile()\n","# X_test = np.ascontiguousarray(X_test)\n","# y_hls = hls_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hls_model.build(csim=False)"]},{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"436a4fce-3843-4c84-8eeb-0161fe3c4e04","_uuid":"4f3a40e23f2e917b68171822944491ab348e15b3","trusted":true},"outputs":[],"source":["sub_df = pd.read_csv(test_path)\n","sub_df['drawing'] = sub_df['drawing'].map(_stack_it)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"72825ea87d35ad96b0254e3af5f5aaf64fb9c78f","trusted":true},"outputs":[],"source":["sub_vec = np.stack(sub_df['drawing'].values, 0)\n","sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=1024)"]},{"cell_type":"code","execution_count":30,"metadata":{"_uuid":"639ca8a511e5e1a02b6cd0333cc04213f8497487","trusted":true},"outputs":[],"source":["top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"68dd3629f5e5b30bede2d4b485a6f1dfabc8d5a4","trusted":true},"outputs":[],"source":["top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\n","top_3_pred[:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6a60baa74045ff401dab7e14dd20710dc4535f67","trusted":true},"outputs":[],"source":["fig, m_axs = plt.subplots(3,3, figsize = (96, 64))\n","rand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\n","for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n","    test_arr = sub_vec[c_id]\n","    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n","    lab_idx = np.cumsum(test_arr[:,2]-1)\n","    for i in np.unique(lab_idx):\n","        c_ax.plot(test_arr[lab_idx==i,0], \n","                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n","    c_ax.axis('off')\n","    c_ax.set_title(top_3_pred[c_id])"]},{"cell_type":"code","execution_count":33,"metadata":{"_uuid":"2b5ece83cb6095e95ef5741e73508d9129be1e3d","trusted":true},"outputs":[],"source":["sub_df['word'] = top_3_pred\n","sub_df[['key_id', 'word']].to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"366a5a7b8bbf29317bb182d46bf8d48c730c440c","trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":1}
